{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que o twitter está pensando?\n",
    "\n",
    "# Extraindo informações em redes sociais utilizando Python\n",
    "\n",
    "`> por: `[@profadolfoguimaraes](http://www.twitter.com/profadolfoguimaraes)\n",
    "\n",
    "Estes tutoriais apresentam os principais scritps desenvolvidos no minicurso: **O que o twitter está pensando? Extraindo informações em redes sociais utilizando Python**. O conteúdo está dividio em dois repositórios: (1) [d2l-minicursotwitter-notebook](http://github.com/adolfoguimaraes/d2l-minicursotwitter-notebook) que possui estes notebooks e (2) [d2l-minicursotwitter-web](http://github.com/adolfoguimaraes/d2l-minicursotwitter-web) que possui a página web desenvolvida.\n",
    "\n",
    "O material completo do minicurso pode ser encontrado em: http://www.data2learning.com/cursos.\n",
    "\n",
    "## 04 - Juntando tudo\n",
    "\n",
    "Ao longo deste tutorial aprendemos como selecionar dados do twitter e fazer algumas etapas de pré-processamento. O objetivo desta seção é juntar tudo. Vamos coletar dados do twitter, pré-processar e imprimir a lista de tokens mais frequentes para um conjunto de dados coletados.\n",
    "\n",
    "* O primeiro passo é coletar tweets e adicionar em uma lista. Para nossa tarefa só interessa o texto e o login do usuário que está postando. Demais informações podem ser descartadas.\n",
    "* Em seguida vamos processar os textos coletados tokenizando.\n",
    "* Deve ser armazenado uma lista das hashtags mais postadas e dos usuários que mais postaram.\n",
    "* Em seguida devemos eliminar as stopwords.\n",
    "* Devemos eliminar eventuais problemas no texto: palavras repetidas e escritas de forma incorreta.\n",
    "* Por fim, extrair os 50 termos mais frequentes usando bigram e trigram.\n",
    "* Ao final deve imprimir a lista dos termos, a lista das hashtags e a lista dos usuários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "\n",
    "from twython import Twython\n",
    "from datetime import datetime\n",
    "\n",
    "#Definição das chaves da API do Twitter\n",
    "consumer_key = None # Get Keys and Access Token at apps.twitter.com\n",
    "consumer_secret = None # Get Keys and Access Token at apps.twitter.com\n",
    "access_token = None # Get Keys and Access Token at apps.twitter.com\n",
    "access_token_secret = None # Get Keys and Access Token at apps.twitter.com\n",
    "\n",
    "tw = Twython(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "\n",
    "result = tw.search(q=\"vigadores\",count=100,lang=\"pt\")\n",
    "\n",
    "tweets = result['statuses']\n",
    "\n",
    "list_text = []\n",
    "list_user = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    #Evita pegar texto truncado no caso de Retweeteds\n",
    "    if 'retweeted_status' in tweet.keys():\n",
    "        list_text.append(tweet['retweeted_status']['text'])\n",
    "    else:\n",
    "        list_text.append(tweet['text'])\n",
    "        \n",
    "    list_user.append(tweet['user']['screen_name'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As variáveis **list_text** e **list_user** armazenam a lista de todos os textos e usuários, respectivamente. O passo seguinte é tratar essas listas com as etapas de pré-processamento.\n",
    "\n",
    "Vamos tratar todos os tweets como um único texto e fazer o pré-processamento nesse texto. A partir do texto vamos extrair a lista de palavras mais citadas, lista de usuários citados e lista de hashtags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unicodedata import normalize, category\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from collections import Counter, Set\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "pattern = r'(https://[^\"\\' ]+|www.[^\"\\' ]+|http://[^\"\\' ]+|\\w+|\\@\\w+|\\#\\w+)'\n",
    "portuguese_stops = stopwords.words(['portuguese'])\n",
    "\n",
    "users_cited = []\n",
    "links_appears = []\n",
    "hashtags = []\n",
    "\n",
    "patterns = []\n",
    "\n",
    "for text in list_text:\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    patterns += regexp_tokenize(text, pattern)\n",
    "\n",
    "    users_cited += [e for e in patterns if e[0] == '@']\n",
    "    links_appears += [e for e in patterns if e[:4] == 'http']\n",
    "    hashtags += [e for e in patterns if e[0] == '#']\n",
    "    \n",
    "    final_tokens = [e for e in patterns if e[:4] != 'http']\n",
    "    final_tokens = [e for e in final_tokens if e[:4] != 'www.']\n",
    "    final_tokens = [e for e in final_tokens if e[0] != '#']\n",
    "    final_tokens = [e for e in final_tokens if e[0] != '@']\n",
    "    \n",
    "    \n",
    "words = [word for word in final_tokens if word not in portuguese_stops]\n",
    "\n",
    "word_set = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Classes para correção\n",
    "import re\n",
    "\n",
    "class RepeatReplacer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cria um map das palavras erradas para as palavras corrigidas\n",
    "\n",
    "replacer_repeat = RepeatReplacer()\n",
    "\n",
    "new_words = []\n",
    "map_words = {}\n",
    "for word in word_set:\n",
    "    \n",
    "    new_word = replacer_repeat.replace(word)\n",
    "    \n",
    "    map_words[word] = new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_temp = [map_words[word] for word in words ]\n",
    "words_temp = [word for word in words_temp if len(word) >= 3]\n",
    "\n",
    "final_words = []\n",
    "\n",
    "for word in words_temp:\n",
    "    try:\n",
    "        new_word = normalize('NFKD', word.lower()).encode('ASCII','ignore')\n",
    "    except UnicodeEncodeError:\n",
    "        new_word = normalize('NFKD', word.lower().decode('utf-8')).encode('ASCII','ignore')\n",
    "\n",
    "    final_words.append(new_word.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vigadores\n",
      "guera\n",
      "infinita\n",
      "ver\n",
      "filme\n",
      "pra\n",
      "asistir\n",
      "sobre\n",
      "spoiler\n",
      "asisti\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "frequence_terms = nltk.FreqDist(final_words)\n",
    "\n",
    "for word in frequence_terms.most_common(10):\n",
    "    print(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rmadridybala', 2), ('JessiFeijoli', 2), ('Carol_Castro_B', 2), ('medicilouis', 2), ('WiliamCruzdaSi1', 2), ('narryhelike', 1), ('madorattioto', 1), ('Carolll1906', 1), ('Crystopher_Kau', 1), ('SamOdev', 1)]\n",
      "[('@anatuzino_', 90), ('@twitter', 85), ('@n3rdizito', 82), ('@leticiaa_rl', 61), ('@crfnicole', 52), ('@legadodacopa', 40), ('@peterjordan100', 39), ('@nathalialivya', 37), ('@marcioroocha', 35), ('@renatoestranho', 25)]\n",
      "[('#quintadetremurasdv', 94), ('#vigadores', 85), ('#guerrainfinita', 85), ('#quartadetremurasdv', 65), ('#diadedecepcao', 26), ('#fanaccountsdv', 7)]\n"
     ]
    }
   ],
   "source": [
    "# Retornando os usuários, usuários citados e hashtags mais frequentes\n",
    "frequence_users = nltk.FreqDist(list_user)\n",
    "frequence_users_cited = nltk.FreqDist(users_cited)\n",
    "frequence_hashtags = nltk.FreqDist(hashtags)\n",
    "\n",
    "print(frequence_users.most_common(10))\n",
    "print(frequence_users_cited.most_common(10))\n",
    "print(frequence_hashtags.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vigadores', 'guera')\n",
      "('guera', 'infinita')\n",
      "('ver', 'vigadores')\n",
      "('asistir', 'vigadores')\n",
      "('sobre', 'vigadores')\n",
      "('vigadores', 'guera', 'infinita')\n",
      "('sobre', 'vigadores', 'guera')\n",
      "('aguento', 'ver', 'coisas')\n",
      "('coisas', 'sobre', 'vigadores')\n",
      "('guera', 'infinita', 'chega')\n"
     ]
    }
   ],
   "source": [
    "#Pegando os bigram e trigram mais frequentes\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(final_words)\n",
    "tcf = TrigramCollocationFinder.from_words(final_words)\n",
    "\n",
    "bcf.apply_freq_filter(3)\n",
    "tcf.apply_freq_filter(3)\n",
    "\n",
    "result_bi = bcf.nbest(BigramAssocMeasures.raw_freq, 5)\n",
    "result_tri = tcf.nbest(TrigramAssocMeasures.raw_freq, 5)\n",
    "\n",
    "\n",
    "for r in result_bi:\n",
    "    print(r)\n",
    "print\n",
    "for r in result_tri:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso finalizamos nossa parte de pré-processamento de texto. Observe que só extraindo de forma correta os termos mais frequentes temos uma visão melhor do que está sendo discutido nas redes sociais. No entanto, exibir isso somente na linha de comando não é muito atrativo. O próximo passo é exibir estas informações em uma página web.\n",
    "\n",
    "Os arquivos da página web criada podem ser acessados no repositório: [d2l-minicursotwitter-web](http://www.github.com/adolfoguimaraes/d2l-minicursotwitter-web)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
