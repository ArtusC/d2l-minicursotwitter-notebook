{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que o twitter está pensando?\n",
    "\n",
    "# Extraindo informações em redes sociais utilizando Python\n",
    "\n",
    "`> por: `[@profadolfoguimaraes](http://www.twitter.com/profadolfoguimaraes)\n",
    "\n",
    "Estes tutoriais apresentam os principais scritps desenvolvidos no minicurso: **O que o twitter está pensando? Extraindo informações em redes sociais utilizando Python**. O conteúdo está dividio em dois repositórios: (1) [d2l-minicursotwitter-notebook](http://github.com/adolfoguimaraes/d2l-minicursotwitter-notebook) que possui estes notebooks e (2) [d2l-minicursotwitter-web](http://github.com/adolfoguimaraes/d2l-minicursotwitter-web) que possui a página web desenvolvida.\n",
    "\n",
    "O material completo do minicurso pode ser encontrado em: http://www.data2learning.com/cursos.\n",
    "\n",
    "## 03 - Pré-processamento de texto utilizando NLTK\n",
    "\n",
    "Nesta etapa vamos aprender algumas técnicas para tratar os textos que foram coletados do twitter. Incialmente vamos trabalhar com a parte de tokenização, remoção de palavras que não serão úteis, aplicação de métodos de *steamming* e retirada automática de links, hashtags do texto e outras informações relevantes que vão ser detalhadas mais a frente.\n",
    "\n",
    "O NLTK (http://www.nltk.org) não é uma ferramenta de pré-processamento, ele é um kit completo para tratar problemas relacionados ao processamento de Linguagem Natural. No entanto, ele possui alguns métodos para nos ajudar nestas tarefas iniciais. Citanto o site oficial: *NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.*\n",
    "\n",
    "O material desenvolvido aqui é baseado, principalmente, no livro **Python 3 Text Processing with NLTK 3 Cookbook**: https://www.amazon.com.br/dp/B00N2RWMJU/.\n",
    "\n",
    "Vamos começar :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação do NLTK\n",
    "\n",
    "Para instalar utilize o comando **pip install nltk** na linha de comando. Se já instalou a partir do arquivo `requirements.txt` não precisa executar esse comando. \n",
    "\n",
    "Esse comando garante a instalação básica da ferramenta. No entanto, o NLTK é composto por uma série de bases, corporas e modelos que podem ser baixados a medida que forem sendo utilizandos. Detalhes de como instalar os dados podem ser encontrados em: http://nltk.org/data.html. A medida que a gente for precisando destes dados, iremos instalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização\n",
    "\n",
    "A primeira etapa do pré-processamento do texto coletado é separá-lo em *tokens*. Tokenizar consiste em quebrar um texto em pedaços, sejam palavras ou sentenças. Para utilizar os métodos desta etapa será necessário instalar alguns itens adicionais do NLTK.\n",
    "\n",
    "Para isso, digite na linha de comando:\n",
    "\n",
    "\n",
    "$ python\n",
    "\n",
    "\n",
    "No shell do Python digite os comandos a seguir:\n",
    "\n",
    "```python\n",
    ">>> import ntlk\n",
    ">>> nltk.download()\n",
    "NLTK Downloader                                                                                                                                       \n",
    "---------------------------------------------------------------------------                                                                           \n",
    "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit                                                                                \n",
    "---------------------------------------------------------------------------                                                                           \n",
    "Downloader> \n",
    "```\n",
    "\n",
    "Nessa tela, se digitar **l** será listado todos os pacotes que podem ser instalados. Para o nosso propósito de tokenização vamos instalar o pacote **punkt** que possui os modelos necessários para que a gente faça os diversos tipos de tokenização.\n",
    "\n",
    "```python\n",
    "Downloader> d punkt                                                                                                                                   \n",
    "    Downloading package punkt to /home/d2l/nltk_data...                                                               \n",
    "        Unzipping tokenizers/punkt.zip. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de instalação de itens adicionais do NLKT pode ser feito diretamente aqui no Jupyter notebook digitando o seguinte código na célula: \n",
    "    \n",
    "```python\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "Vai aparecer uma caixa onde deve ser digitado o que deve ser baixado como mostrado no exemplo anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir utiliza um modelo simples de tokenização baseado nas palavras. Observe que ele considera as potuações como uma palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'que', 'o', 'twitter', 'esta', 'pensando', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(\"O que o twitter esta pensando.\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o mesmo método em outra frase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Para', 'mais', 'informações', 'sobre', 'o', 'minicurso', ',', 'acesse', ':', 'http', ':', '//www.data2learning.com/cursos']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"Para mais informações sobre o minicurso, acesse: http://www.data2learning.com/cursos\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou ainda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'meu', 'perfil', '@', 'profadolfoguimaraes', 'no', 'instagram', 'para', ';', ')', '#', 'minicursotwitterludiico']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"Siga meu perfil @profadolfoguimaraes no instagram para ;) #minicursotwitterludiico\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O NLTK possui uma série de métodos de tokenização: *PunktWordTokenizer*, *RegexpTokenizer*, *TreebankWordTokenizer* e *TweetTokenizer* Cada um destes métodos utilizam abordagens diferentes para separar os tokens. Devemos escolher o método mais adequado a depender do objtivo que queremos alcançar. A lista completa de métodos de tokenização, pode ser encontrada na documentação do NLTK: http://www.nltk.org/api/nltk.tokenize.html.\n",
    "\n",
    "No nosso caso, observe que objetivo foi em parte atingido. Algumas partes da tokenização não foram bem sucedidas. Termos como links, hashtags e nomes de usuários não deviam ser separados dos 'http', '@' ou '#' correspondentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O NLTK possui um tokenizador próprio para tweets: `TweetTokenizer`. Veja no exemplo a seguir que ele agrupa como uma única palavra nomes de usuários, hashtags, links e *emotions*. Esse resultado é muito mais próximo do que queremos aqui. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'meu', 'perfil', '@profadolfoguimaraes', 'no', 'instagram', 'para', ';)', '#minicursotwitterludiico', 'http://www.instagram.com/profadolfoguimaraes', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweettk = TweetTokenizer()\n",
    "words = tweettk.tokenize(\"Siga meu perfil @profadolfoguimaraes no instagram para ;) #minicursotwitterludiico \\\n",
    "                         http://www.instagram.com/profadolfoguimaraes.\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma mais genérica de resolvermos esse problema, é utilizando uma tokenização baseada em expressão regular. Para este tipo, escrevemos uma expressão regular que permita reconhecer os padrões que queremos separar. \n",
    "\n",
    "O NLTK possui o *RegexpTokenizer* para fazer tal tarefa. Esse método faz o que a função básica do python *re.findall()* do pacote *re* faz. No entanto, alguns métodos do NLTK exigem que seja definido um expressão regular para reconhecer determinados padrões de texto. Ter uma forma própria de implementar as expressões regulares facilita na definição destes métodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir quebra uma frase a partir da expressão regular **[\\w]+**. A expressão diz que o padrão reconhecido deve ser formado por uma ou mais letras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'que', 'o', 'twitter', 'esta', 'pensando']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "expression = \"[\\w]+\"\n",
    "text = \"O que o twitter esta pensando.\"\n",
    "patterns = regexp_tokenize(text, expression)\n",
    "\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o ponto não foi selecionado nesse tipo de tokenização já que ele não casa com o padrão desejado. Se utilizarmos esse mesmo padrão nos textos que possuem links, hashtags ou nome de usuários ainda não vamos obter o desejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'meu', 'perfil', 'profadolfoguimaraes', 'no', 'instagram', 'para', 'minicursotwitterludiico', 'http', 'www', 'instagram', 'com', 'profadolfoguimaraes']\n"
     ]
    }
   ],
   "source": [
    "text = \"Siga meu perfil @profadolfoguimaraes no instagram para ;) #minicursotwitterludiico \\\n",
    "                         http://www.instagram.com/profadolfoguimaraes\"\n",
    "patterns = regexp_tokenize(text, expression)\n",
    "\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos elaborar um padrão mais completo. Cada \"casamento\" do padrão será reconhecido como um token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'meu', 'perfil', '@profadolfoguimaraes', 'no', 'instagram', 'para', '#minicursotwitterludiico', 'http://www.instagram.com/profadolfoguimaraes']\n"
     ]
    }
   ],
   "source": [
    "#Mudando o padrão\n",
    "\n",
    "pattern = r'(https://[^\"\\' ]+|www.[^\"\\' ]+|http://[^\"\\' ]+|\\w+|\\@\\w+|\\#\\w+)'\n",
    "\n",
    "patterns = regexp_tokenize(text, pattern)\n",
    "\n",
    "print(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A documentação para formar expressões regulares pode ser encontrada na biblioteca **re** do Python: \n",
    "\n",
    "* https://docs.python.org/2/library/re.html (Python 2.7+)\n",
    "* https://docs.python.org/3/library/re.html (Python 3+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Uma outra etapa de pré-processamento é a de eliminação das stopwords. **Stopwords** são palavras comuns que de forma geral não contribuem para o entendimento da senteça. Vale reforçar que isso vai depender do propósito da sua aplicação. Para o nosso caso, podemos eliminar sem problemas.\n",
    "\n",
    "O NLTK vem com uma lista de stopwords para diversas linguagens. Para poder utilizar esse pacote devemos instalar também utilizando **nltk.download()** exemplificado anteriormente.\n",
    "\n",
    "```python\n",
    ">>> import nltk                                                                                                                                       \n",
    ">>> nltk.download()                                                                                                                                   \n",
    "NLTK Downloader                                                                                                                                       \n",
    "---------------------------------------------------------------------------                                                                           \n",
    "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit                                                                                \n",
    "---------------------------------------------------------------------------                                                                           \n",
    "Downloader> d stopwords                                                                                                                               \n",
    "    Downloading package stopwords to /home/d2l/nltk_data...                                                                                           \n",
    "      Unzipping corpora/stopwords.zip.    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar algumas listas de palavras. Linguagens disponíveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'kazakh', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos exibir algumas palavras (para simplificar, iremos exibir apenas 10 de cada conjunto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']\n",
      "['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para']\n",
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n"
     ]
    }
   ],
   "source": [
    "english_stops = stopwords.words(['english'])\n",
    "portuguese_stops = stopwords.words(['portuguese'])\n",
    "spanish_stops = stopwords.words(['spanish'])\n",
    "print(english_stops[:10])\n",
    "print(portuguese_stops[:10])\n",
    "print(spanish_stops[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso podemos eliminar as stopwords do nosso texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Siga', 'meu', 'perfil', '@profadolfoguimaraes', 'no', 'instagram', 'para', '#minicursotwitterludiico', 'http://www.instagram.com/profadolfoguimaraes']\n",
      "['Siga', 'perfil', '@profadolfoguimaraes', 'instagram', '#minicursotwitterludiico', 'http://www.instagram.com/profadolfoguimaraes']\n"
     ]
    }
   ],
   "source": [
    "text = \"Siga meu perfil @profadolfoguimaraes no instagram para ;) #minicursotwitterludiico \\\n",
    "                         http://www.instagram.com/profadolfoguimaraes\"\n",
    "\n",
    "patterns = regexp_tokenize(text, pattern)\n",
    "\n",
    "print(patterns)\n",
    "\n",
    "words = [word for word in patterns if word not in portuguese_stops]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo correções\n",
    "\n",
    "Um problema muito comum quando trabalhamos com textos de redes sociais são os frequntes problemas com a escrita das palavras. Dentre vários problemas que podemos identificar, um que acontece com certa frequência é a inclusão de caracteres repetidos em uma palavra para enfatizar alguma coisa. Por exemplo, \"gostei muuuuiiitoooo desse filme\". O computador não sabe diferenciar \"muito\" de \"muuuuiiitoooo\". Para o processamento de texto seria importante que a gente corrigisse tais casos. A classe a seguir usa a biblioteca **re** do python para elimiar esse tipo de erro. \n",
    "\n",
    "Mais uma vez vale ressaltar que tais correções depende da aplicação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class RepeatReplacer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "        \n",
    "    def replace(self, word):\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar a classe definida para corrigir algumas palavras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "muito\n",
      "voce\n"
     ]
    }
   ],
   "source": [
    "replacer = RepeatReplacer()\n",
    "\n",
    "word = replacer.replace('muuuuiiitoooo')\n",
    "\n",
    "print(word)\n",
    "\n",
    "word = replacer.replace('voceeeee')\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **AVISO:** Um outro tipo de correção é fazer a correção ortográfica. Inicialmente, havia utilizado a biblioteca **pyenchant**, mas ela foi descontinuada pelos seus desenvolvedores. Resolvi retirar desse material. Caso encontre outras alternativas irei atualiza-lo com essa parte. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termos mais frequentes\n",
    "\n",
    "Todos os processos feitos até aqui tem como finalidade ajustar as palavras para que a gente possa extrair de forma correta os termos mais frequentes. Selecionar os termos frequentes permite ter uma visão dos principais termos que estão presentes em um texto. O NLTK possui um método que retorna a frequencia de cada palavrada, dado um conjunto de palavras.\n",
    "\n",
    "As tarefas feitas anteriomente faz com que **voce** e **voceeeee** sejam tratadas como a mesma palavra. O que de fato é (para o escopo que estamos trabalhando). A mesma coisa acontece para palavras escritas de forma errada.\n",
    "\n",
    "Uma outra coisa que é importante fazer antes de determinar a frequencia das palavras é a retirada de acentos e normalizar todas as palavras com letra minúscula. \n",
    "\n",
    "O código a seguir faz estas tarefas e calcula a frequência dos termos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('siga', 2), ('meu', 2), ('perfil', 2), ('de', 2), ('#ludiico', 2), ('@profadolfoguimaraes', 1), ('no', 1), ('instagram', 1), ('para', 1), ('#minicursotwitterludiico', 1), ('http://www.instagram.com/profadolfoguimaraes\\nquero', 1), ('participar', 1), ('todos', 1), ('os', 1), ('minicursos', 1), ('como', 1), ('faz', 1), ('em', 1), ('breve', 1), ('mais', 1), ('iniciativas', 1), ('do', 1), ('grupo', 1), ('pesquisa', 1), ('que', 1), ('irei', 1), ('divulga', 1), ('las', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Importar pacote para normalizar o text\n",
    "import nltk\n",
    "from unicodedata import normalize\n",
    "\n",
    "# Textos a serem tratados\n",
    "text1 = \"Siga meu perfil @profadolfoguimaraes no instagram para ;) #minicursotwitterludiico \\\n",
    "                         http://www.instagram.com/profadolfoguimaraes\"\n",
    "text2 = \"Quero participar de todos os minicursos. Como faz? #ludiico\"\n",
    "text3 = \"Em breve, mais iniciativas do grupo de pesquisa #ludiico. Siga meu perfil que irei divulga-las.\"\n",
    "\n",
    "# Concatenação dos textos\n",
    "text = text1 + \"\\n\" + text2 + \"\\n\" + text3\n",
    "\n",
    "# Retirada dos acentos\n",
    "text = normalize('NFKD', text)\n",
    "\n",
    "# Colaca todas as palavras em minúscula\n",
    "text = text.lower()\n",
    "\n",
    "# Processo de tokenização\n",
    "patterns = regexp_tokenize(text, pattern)\n",
    "\n",
    "# Cálculo dos termos mais frequentes\n",
    "frequence_terms = nltk.FreqDist(patterns)\n",
    "    \n",
    "#Podemos ordenar os termos de acordo com a frequência    \n",
    "print(frequence_terms.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "\n",
    "Quando trabalhamos com texto algumas palavras tendem a aparecer juntas. Por exemplo, *United States*, *Barak Obama*, *Hermes Fontes*, *Augusto Franco* e assim por diante. Identificar esses termos é importante dentro do processamento de linguagem natural. Por exemplo, *Hermes Fontes* é um nome de avenida. Desta forma, estas palavras quando aparecem juntas trazem muito mais informação do que as palavras soltas: hermes, fontes. O nome geral para isto é *n-gram*, onde *n* é o número de palavras que vão aparecer juntas. Os mais utilizados são n = 2 ou n = 3. Estes casos recebem o nome de `bigram` e `trigram`, respectivamente.\n",
    "\n",
    "Vamos trabalhar com as frases vistas anteriomente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('meu', 'perfil'), ('siga', 'meu'), ('#minicursotwitterludiico', 'http://www.instagram.com/profadolfoguimaraes\\nquero'), ('@profadolfoguimaraes', 'no')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "bcf = BigramCollocationFinder.from_words(patterns)\n",
    "\n",
    "result = bcf.nbest(BigramAssocMeasures.likelihood_ratio, 4)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma podemos implementar para o trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('siga', 'meu', 'perfil'), ('meu', 'perfil', '@profadolfoguimaraes'), ('meu', 'perfil', 'que'), ('#ludiico', 'siga', 'meu'), ('#minicursotwitterludiico', 'http://www.instagram.com/profadolfoguimaraes\\nquero', 'participar')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "bcf = TrigramCollocationFinder.from_words(patterns)\n",
    "\n",
    "result = bcf.nbest(TrigramAssocMeasures.likelihood_ratio, 5)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importância em selecionar estes padrões pode ser enxergado melhor com uma base muito maior de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso encerramos nossa etapa de pré-processamento. Existe muito mais coisas que podem ser feitas como *steamming*, *identificação de sinônimos*, entre outros. Um guia completo destas tarefas podem ser encontradas no livo do NLTK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
